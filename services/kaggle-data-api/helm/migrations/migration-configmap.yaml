apiVersion: v1
kind: ConfigMap
metadata:
  name: kaggle-data-api-migrations
  namespace: super-zol
  labels:
    app: kaggle-data-api
    component: migrations
data:
  001_create_datasets_table.sql: |
    -- Migration: Create datasets table
    -- Description: Creates the datasets table with mock data for testing

    -- Drop table if exists (for development)
    DROP TABLE IF EXISTS datasets CASCADE;

    -- Create datasets table
    CREATE TABLE datasets (
        id SERIAL PRIMARY KEY,
        name VARCHAR(255) NOT NULL UNIQUE,
        title VARCHAR(500) NOT NULL,
        description TEXT,
        url VARCHAR(1000) NOT NULL,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );

    -- Create index on name for faster lookups
    CREATE INDEX idx_datasets_name ON datasets(name);

    -- Create index on created_at for sorting
    CREATE INDEX idx_datasets_created_at ON datasets(created_at DESC);

    -- Insert mock data for testing
    INSERT INTO datasets (name, title, description, url) VALUES
    (
        'titanic',
        'Titanic: Machine Learning from Disaster',
        'Start here! Predict survival on the Titanic and get familiar with ML basics',
        'https://www.kaggle.com/c/titanic'
    ),
    (
        'house-prices-advanced-regression-techniques',
        'House Prices - Advanced Regression Techniques',
        'Predict sales prices and practice feature engineering, RFs, and gradient boosting',
        'https://www.kaggle.com/c/house-prices-advanced-regression-techniques'
    ),
    (
        'digit-recognizer',
        'Digit Recognizer',
        'Learn computer vision fundamentals with the famous MNIST data',
        'https://www.kaggle.com/c/digit-recognizer'
    ),
    (
        'nlp-getting-started',
        'Natural Language Processing with Disaster Tweets',
        'Predict which Tweets are about real disasters and which ones are not',
        'https://www.kaggle.com/c/nlp-getting-started'
    ),
    (
        'spaceship-titanic',
        'Spaceship Titanic',
        'Predict which passengers are transported to an alternate dimension',
        'https://www.kaggle.com/competitions/spaceship-titanic'
    ),
    (
        'store-sales-time-series-forecasting',
        'Store Sales - Time Series Forecasting',
        'Use machine learning to predict grocery sales',
        'https://www.kaggle.com/c/store-sales-time-series-forecasting'
    ),
    (
        'contradictory-my-dear-watson',
        'Contradictory, My Dear Watson',
        'Detect contradiction and entailment in multilingual text using TPUs',
        'https://www.kaggle.com/c/contradictory-my-dear-watson'
    ),
    (
        'playground-series-s4e1',
        'Regression with a Crab Age Dataset',
        'Predict the age of crabs based on various features',
        'https://www.kaggle.com/competitions/playground-series-s4e1'
    ),
    (
        'dogs-vs-cats',
        'Dogs vs. Cats',
        'Create an algorithm to distinguish dogs from cats',
        'https://www.kaggle.com/c/dogs-vs-cats'
    ),
    (
        'tabular-playground-series-jan-2021',
        'Tabular Playground Series - Jan 2021',
        'Predict a continuous target based on a set of feature variables',
        'https://www.kaggle.com/c/tabular-playground-series-jan-2021'
    );

    -- Create a function to update updated_at timestamp
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = CURRENT_TIMESTAMP;
        RETURN NEW;
    END;
    $$ language 'plpgsql';

    -- Create trigger to auto-update updated_at
    CREATE TRIGGER update_datasets_updated_at
        BEFORE UPDATE ON datasets
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at_column();

    -- Verify the data
    SELECT COUNT(*) as total_datasets FROM datasets;

  run-migrations.sh: |
    #!/usr/bin/env bash
    #
    # Database migration runner
    # Runs all SQL migration files in order

    set -euo pipefail

    # Configuration
    DB_HOST="${DB_HOST:-postgresql.super-zol.svc.cluster.local}"
    DB_PORT="${DB_PORT:-5432}"
    DB_NAME="${DB_NAME:-superzol}"
    DB_USER="${DB_USER:-superzol}"
    DB_PASSWORD="${DB_PASSWORD:-}"

    # Colors
    GREEN='\033[0;32m'
    RED='\033[0;31m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color

    log_info() {
        echo -e "${YELLOW}[INFO]${NC} $1"
    }

    log_success() {
        echo -e "${GREEN}[✓]${NC} $1"
    }

    log_error() {
        echo -e "${RED}[✗]${NC} $1"
    }

    # Get script directory
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

    log_info "Starting database migrations..."
    log_info "Host: ${DB_HOST}:${DB_PORT}"
    log_info "Database: ${DB_NAME}"
    log_info "User: ${DB_USER}"

    # Check if password is provided
    if [ -z "$DB_PASSWORD" ]; then
        log_error "DB_PASSWORD environment variable is not set"
        exit 1
    fi

    # Export password for psql
    export PGPASSWORD="$DB_PASSWORD"

    # Test database connection
    log_info "Testing database connection..."
    if psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -c "SELECT 1" >/dev/null 2>&1; then
        log_success "Database connection successful"
    else
        log_error "Failed to connect to database"
        exit 1
    fi

    # Run migrations in order
    log_info "Running migrations..."

    for migration_file in "$SCRIPT_DIR"/*.sql; do
        if [ -f "$migration_file" ]; then
            migration_name=$(basename "$migration_file")
            log_info "Running migration: $migration_name"

            if psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$migration_file"; then
                log_success "Migration completed: $migration_name"
            else
                log_error "Migration failed: $migration_name"
                exit 1
            fi
        fi
    done

    log_success "All migrations completed successfully!"

    # Show dataset count
    log_info "Verifying data..."
    DATASET_COUNT=$(psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COUNT(*) FROM datasets" | xargs)
    log_success "Total datasets in database: $DATASET_COUNT"

    exit 0
